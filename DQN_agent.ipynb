{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN agent.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4RA8_uPkiWs"
      },
      "source": [
        "installing tensorflow, tensortrade and technical analysis libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2Bgj2vfgJQW4",
        "outputId": "3d21bdd3-bf6b-4f6d-ef19-6e57a4976569"
      },
      "source": [
        "!pip install tf\n",
        "!pip install tensortrade\n",
        "!pip3 install ta"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf\n",
            "  Downloading tf-1.0.0.tar.gz (620 bytes)\n",
            "Building wheels for collected packages: tf\n",
            "  Building wheel for tf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tf: filename=tf-1.0.0-py3-none-any.whl size=1285 sha256=5f84d74b012d5655e84e3d3d08851f1c95e6999e19274b9c2555998a41c67a31\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/c7/58/cca67875b41ff853d3fdaa20b54a780ef2e045fbcacaef1ee3\n",
            "Successfully built tf\n",
            "Installing collected packages: tf\n",
            "Successfully installed tf-1.0.0\n",
            "Collecting tensortrade\n",
            "  Downloading tensortrade-1.0.3.tar.gz (32.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 32.6 MB 17 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from tensortrade) (1.19.5)\n",
            "Requirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.7/dist-packages (from tensortrade) (1.1.5)\n",
            "Requirement already satisfied: gym>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from tensortrade) (0.17.3)\n",
            "Collecting pyyaml>=5.1.2\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 14.2 MB/s \n",
            "\u001b[?25hCollecting stochastic>=0.6.0\n",
            "  Downloading stochastic-0.6.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tensortrade) (2.6.0)\n",
            "Collecting ipython>=7.12.0\n",
            "  Downloading ipython-7.28.0-py3-none-any.whl (788 kB)\n",
            "\u001b[K     |████████████████████████████████| 788 kB 34.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.7/dist-packages (from tensortrade) (3.2.2)\n",
            "Collecting plotly>=4.5.0\n",
            "  Downloading plotly-5.3.1-py2.py3-none-any.whl (23.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.9 MB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.14.0->tensortrade) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.14.0->tensortrade) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.14.0->tensortrade) (1.4.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.12.0->tensortrade) (0.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.12.0->tensortrade) (0.2.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.12.0->tensortrade) (57.4.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.12.0->tensortrade) (4.8.0)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.20-py3-none-any.whl (370 kB)\n",
            "\u001b[K     |████████████████████████████████| 370 kB 43.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.12.0->tensortrade) (5.1.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython>=7.12.0->tensortrade) (0.1.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.12.0->tensortrade) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.12.0->tensortrade) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.12.0->tensortrade) (0.7.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.12.0->tensortrade) (0.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->tensortrade) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->tensortrade) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->tensortrade) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->tensortrade) (1.3.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=3.1.1->tensortrade) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->tensortrade) (2018.9)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.12.0->tensortrade) (0.7.0)\n",
            "Collecting tenacity>=6.2.0\n",
            "  Downloading tenacity-8.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.12.0->tensortrade) (0.2.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.14.0->tensortrade) (0.16.0)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (5.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (0.2.0)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (2.6.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (0.37.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (1.1.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (1.40.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (1.12.1)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (1.1.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (2.6.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (3.17.3)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (2.6.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (3.7.4.3)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (0.4.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (0.12.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (1.12)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tensortrade) (1.6.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow>=2.1.0->tensortrade) (1.5.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->tensortrade) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->tensortrade) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->tensortrade) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->tensortrade) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->tensortrade) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->tensortrade) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->tensortrade) (0.4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->tensortrade) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->tensortrade) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->tensortrade) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.1.0->tensortrade) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.1.0->tensortrade) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->tensortrade) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.1.0->tensortrade) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.1.0->tensortrade) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.1.0->tensortrade) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.1.0->tensortrade) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.1.0->tensortrade) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.1.0->tensortrade) (3.5.0)\n",
            "Building wheels for collected packages: tensortrade\n",
            "  Building wheel for tensortrade (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensortrade: filename=tensortrade-1.0.3-py3-none-any.whl size=134871 sha256=5a996ef589374fdab0f806689d2599aba004980e716abfcb27d141f3884af494\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/3b/2d/997606a2a1e031b57bc2e463154c8df18c1f9a46f6f5536f3f\n",
            "Successfully built tensortrade\n",
            "Installing collected packages: tenacity, prompt-toolkit, stochastic, pyyaml, plotly, ipython, tensortrade\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: plotly\n",
            "    Found existing installation: plotly 4.4.1\n",
            "    Uninstalling plotly-4.4.1:\n",
            "      Successfully uninstalled plotly-4.4.1\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.20 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.28.0 which is incompatible.\u001b[0m\n",
            "Successfully installed ipython-7.28.0 plotly-5.3.1 prompt-toolkit-3.0.20 pyyaml-5.4.1 stochastic-0.6.0 tenacity-8.0.1 tensortrade-1.0.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ta\n",
            "  Downloading ta-0.7.0.tar.gz (25 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ta) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from ta) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ta) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->ta) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->ta) (1.15.0)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.7.0-py3-none-any.whl size=28718 sha256=bf95dfad34a57b9250c9186882d40f091866df3cdc497bf989305008696a2c1f\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/74/e0/72395003bd1d3c8f3f5860c2d180ff15699e47a2733d8ebd38\n",
            "Successfully built ta\n",
            "Installing collected packages: ta\n",
            "Successfully installed ta-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89hWChCbk9SU"
      },
      "source": [
        "importing necessary libraries and components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRZSUJMRJbQE"
      },
      "source": [
        "import ta\n",
        "import pandas as pd\n",
        "import tensortrade.env.default as default\n",
        "from tensortrade.data.cdd import CryptoDataDownload\n",
        "from tensortrade.feed.core import Stream, DataFeed, NameSpace\n",
        "from tensortrade.oms.instruments import USD, BTC, ETH, LTC\n",
        "from tensortrade.oms.wallets import Wallet, Portfolio\n",
        "from tensortrade.oms.exchanges import Exchange\n",
        "from tensortrade.oms.services.execution.simulated import execute_order"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5vCgkKIlL_N"
      },
      "source": [
        "Uploading crypto data and technical analysis features( Rsi and moving averages)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKtrzKShOTL5"
      },
      "source": [
        "cdd = CryptoDataDownload()\n",
        "\n",
        "data = cdd.fetch(\"Bitfinex\", \"USD\", \"BTC\", \"1h\")\n",
        "def rsi(price: Stream[float], period: float) -> Stream[float]:\n",
        "    r = price.diff()\n",
        "    upside = r.clamp_min(0).abs()\n",
        "    downside = r.clamp_max(0).abs()\n",
        "    rs = upside.ewm(alpha=1 / period).mean() / downside.ewm(alpha=1 / period).mean()\n",
        "    return 100*(1 - (1 + rs) ** -1)\n",
        "\n",
        "\n",
        "def macd(price: Stream[float], fast: float, slow: float, signal: float) -> Stream[float]:\n",
        "    fm = price.ewm(span=fast, adjust=False).mean()\n",
        "    sm = price.ewm(span=slow, adjust=False).mean()\n",
        "    md = fm - sm\n",
        "    signal = md - md.ewm(span=signal, adjust=False).mean()\n",
        "    return signal\n",
        "\n",
        "\n",
        "features = []\n",
        "for c in data.columns[1:]:\n",
        "    s = Stream.source(list(data[c]), dtype=\"float\").rename(data[c].name) #Creates a stream from an iterable\n",
        "    features += [s]\n",
        "\n",
        "cp = Stream.select(features, lambda s: s.name == \"close\") #Selects a stream closing price streams from the list of streams\n",
        "        \n",
        "\n",
        "features = [\n",
        "    cp.log().diff().rename(\"lr\"),\n",
        "    rsi(cp, period=20).rename(\"rsi\"),\n",
        "    macd(cp, fast=10, slow=50, signal=5).rename(\"macd\")\n",
        "]\n",
        "\n",
        "feed = DataFeed(features)\n",
        "feed.compile() # compiles together streams to be run in an organized manner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8SeUQxdlXgj"
      },
      "source": [
        "Creating environment components(action_scheme,reward_scheme, Portfolio,datafeed,exchange, traded pair..) and creating the environment itself which is a gym object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_ZfZPLAOdR-",
        "outputId": "c14b826e-d11a-4385-ba0f-d5a449b9ed21"
      },
      "source": [
        "bitfinex = Exchange(\"bitfinex\", service=execute_order)(\n",
        "    Stream.source(list(data[\"close\"]), dtype=\"float\").rename(\"USD-BTC\")\n",
        ")\n",
        "\n",
        "portfoliox = Portfolio(USD, [\n",
        "    Wallet(bitfinex, 10000 * USD),\n",
        "    Wallet(bitfinex, 0 * BTC)\n",
        "])\n",
        "\n",
        "\n",
        "renderer_feed = DataFeed([\n",
        "    Stream.source(list(data[\"date\"])).rename(\"date\"),\n",
        "    Stream.source(list(data[\"open\"]), dtype=\"float\").rename(\"open\"),\n",
        "    Stream.source(list(data[\"high\"]), dtype=\"float\").rename(\"high\"),\n",
        "    Stream.source(list(data[\"low\"]), dtype=\"float\").rename(\"low\"),\n",
        "    Stream.source(list(data[\"close\"]), dtype=\"float\").rename(\"close\"),\n",
        "    Stream.source(list(data[\"volume\"]), dtype=\"float\").rename(\"volume\")\n",
        "])\n",
        "\n",
        "from tensortrade.env.default.actions import ManagedRiskOrders,BSH\n",
        "\n",
        "env = default.create(\n",
        "    portfolio=portfoliox,\n",
        "    action_scheme= \"managed-risk\",\n",
        "    reward_scheme=\"risk-adjusted\",\n",
        "    feed=feed,\n",
        "    renderer_feed=renderer_feed,\n",
        "    renderer=default.renderers.PlotlyTradingChart(),\n",
        "    window_size=20\n",
        ")\n",
        "\n",
        "env.observer.feed.next()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'external': {'lr': nan, 'macd': 0.0, 'rsi': nan},\n",
              " 'internal': {'bitfinex:/BTC:/free': 0.0,\n",
              "  'bitfinex:/BTC:/locked': 0.0,\n",
              "  'bitfinex:/BTC:/total': 0.0,\n",
              "  'bitfinex:/BTC:/worth': 0.0,\n",
              "  'bitfinex:/USD-BTC': 8739.0,\n",
              "  'bitfinex:/USD:/free': 10000.0,\n",
              "  'bitfinex:/USD:/locked': 0.0,\n",
              "  'bitfinex:/USD:/total': 10000.0,\n",
              "  'net_worth': 10000.0},\n",
              " 'renderer': {'close': 8739.0,\n",
              "  'date': Timestamp('2018-05-15 06:00:00'),\n",
              "  'high': 8793.0,\n",
              "  'low': 8714.9,\n",
              "  'open': 8723.8,\n",
              "  'volume': 8988053.53}}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9r9MrrImD1M"
      },
      "source": [
        "A function to retrieve the time of an action when its done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiUpRh4Nbctg"
      },
      "source": [
        "def timeofaction(price):\n",
        "        v = cdd.fetch(\"Bitfinex\", \"USD\", \"BTC\", \"1h\")\n",
        "        k=0\n",
        "        for i in v.loc[:,\"close\"]:\n",
        "          k+=1  \n",
        "          if (i == float(price)):\n",
        "            print(v.iloc[k,0]) \n",
        "            break\n",
        "           "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytet2bYpmNig"
      },
      "source": [
        "This is the most important piece of code, this is where the agent is defined, the training function(train) , the gradient descent function, the neural network and the deep Q learning algorithm.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9d_DO9bQBCo"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensortrade.data.cdd import CryptoDataDownload\n",
        "from collections import namedtuple\n",
        "from tensortrade.oms.instruments import ExchangePair\n",
        "from tensortrade.agents import Agent, ReplayMemory\n",
        "from datetime import datetime\n",
        "from itertools import product\n",
        "import logging\n",
        "from abc import abstractmethod\n",
        "from itertools import product\n",
        "from typing import Union, List, Any\n",
        "\n",
        "from gym.spaces import Space, Discrete\n",
        "\n",
        "from tensortrade.core import Clock\n",
        "from tensortrade.env.generic import ActionScheme, TradingEnv\n",
        "from tensortrade.oms.instruments import ExchangePair\n",
        "from tensortrade.oms.orders import (\n",
        "    Broker,\n",
        "    Order,\n",
        "    OrderListener,\n",
        "    OrderSpec,\n",
        "    proportion_order,\n",
        "    risk_managed_order,\n",
        "    TradeSide,\n",
        "    TradeType\n",
        ")\n",
        "from tensortrade.oms.wallets import Portfolio\n",
        "from tensortrade.env.default.actions import ManagedRiskOrders,BSH\n",
        "\n",
        "DQNTransition = namedtuple('DQNTransition', ['state', 'action', 'reward', 'next_state', 'done'])\n",
        "actionscheme = ManagedRiskOrders([0.02, 0.04, 0.06],[0.01, 0.02, 0.03],10, None,TradeType.MARKET,None,0.02,0.00)\n",
        "#actionscheme = BSH(cash=portfoliox.wallets[0],asset=portfoliox.wallets[1])\n",
        "actionscheme.portfolio = portfoliox\n",
        "\n",
        " \n",
        "actionscheme.action_space\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DQNAgents(Agent,ManagedRiskOrders):\n",
        "    def __init__(self,\n",
        "                 env: 'TradingEnv',\n",
        "                 policy_network: tf.keras.Model = None):\n",
        "        self.env = env\n",
        "        self.n_actions = env.action_space.n\n",
        "        self.observation_shape = env.observation_space.shape\n",
        "\n",
        "        self.policy_network = policy_network or self.build_policy_network()\n",
        "\n",
        "        self.target_network = tf.keras.models.clone_model(self.policy_network)\n",
        "        self.target_network.trainable = False\n",
        "        \n",
        "        self.env.agent_id = self.id\n",
        "\n",
        "    def build_policy_network(self):\n",
        "        network = tf.keras.Sequential([\n",
        "            tf.keras.layers.LSTM(40,input_shape=self.observation_shape,return_sequences=True),\n",
        "            tf.keras.layers.Dropout(0.2),\n",
        "            tf.keras.layers.LSTM(40,return_sequences=False),\n",
        "            tf.keras.layers.Dropout(0.2),\n",
        "            tf.keras.layers.Dense(self.n_actions, activation=\"sigmoid\"),\n",
        "            tf.keras.layers.Dense(self.n_actions, activation=\"softmax\")\n",
        "        ])\n",
        "        return network\n",
        "\n",
        "    def restore(self, path: str, **kwargs):\n",
        "        self.policy_network = tf.keras.models.load_model(path)\n",
        "        self.target_network = tf.keras.models.clone_model(self.policy_network)\n",
        "        self.target_network.trainable = False\n",
        "\n",
        "    def save(self, path: str, **kwargs):\n",
        "        episode: int = kwargs.get('episode', None)\n",
        "\n",
        "        if episode:\n",
        "            filename = \"policy_network__\" + self.id[:7] + \"__\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".hdf5\"\n",
        "        else:\n",
        "            filename = \"policy_network__\" + self.id[:7] + \"__\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".hdf5\"\n",
        "\n",
        "        self.policy_network.save(path + filename)\n",
        "\n",
        "    def get_action(self, state: np.ndarray, **kwargs) -> int:\n",
        "        threshold: float = kwargs.get('threshold', 0)\n",
        "        rand = random.random() #select a random number between 0 and 1\n",
        "        if rand < threshold:  # if the number is smaller than the threshold we select an action via exploration.\n",
        "            return np.random.choice(self.n_actions)\n",
        "        else: #if not we exploit the knowledge of our agent to select an action.\n",
        "            return np.argmax(self.policy_network(np.expand_dims(state, 0)))\n",
        "\n",
        "    def apply_gradient_descent(self, memory: ReplayMemory, batch_size: int, learning_rate: float, discount_factor: float):\n",
        "        optimizer = tf.keras.optimizers.Adam(lr=learning_rate) #using Adam optimizer\n",
        "        loss = tf.keras.losses.Huber() #Using the huber loss function for calculating the loss\n",
        "        transitions = memory.sample(batch_size)\n",
        "        batch = DQNTransition(*zip(*transitions))\n",
        "        state_batch = tf.convert_to_tensor(batch.state) #preprocess states by turning them into neural network friendly input(tensors)\n",
        "        action_batch = tf.convert_to_tensor(batch.action)#preprocess actions by turning them into neural network friendly input(tensors)\n",
        "        reward_batch = tf.convert_to_tensor(batch.reward, dtype=tf.float32)#preprocess rewards by turning them into neural network friendly input(tensors)\n",
        "        next_state_batch = tf.convert_to_tensor(batch.next_state)#preprocess next states by turning them into neural network friendly input(tensors)\n",
        "        done_batch = tf.convert_to_tensor(batch.done)#preprocess \"done\" status by turning them into neural network friendly input(tensors)\n",
        "        with tf.GradientTape() as tape:\n",
        "            state_action_values = tf.math.reduce_sum(\n",
        "                self.policy_network(state_batch) * tf.one_hot(action_batch, self.n_actions),\n",
        "                axis=1 ) #We feed a batch of states to the neural network and output the action Q-values\n",
        "            next_state_values = tf.where( done_batch,\n",
        "                tf.zeros(batch_size),\n",
        "                tf.math.reduce_max(self.target_network(next_state_batch), axis=1))# the target network is used for computing the non-stationary target Q-values\n",
        "            expected_state_action_values = reward_batch + (discount_factor * next_state_values)\n",
        "            loss_value = loss(expected_state_action_values, state_action_values)#The loss between Q-values and target Q-values is calculated\n",
        "        variables = self.policy_network.trainable_variables\n",
        "        gradients = tape.gradient(loss_value, variables) #Calculate the gradient of the loss function\n",
        "        optimizer.apply_gradients(zip(gradients, variables))#Apply gradient descent on the loss fucntion.\n",
        "         \n",
        "    \n",
        "    def train(self,\n",
        "              n_steps: int = None,\n",
        "              n_episodes: int = None,\n",
        "              save_every: int = None,\n",
        "              save_path: str = None,\n",
        "              callback: callable = None,\n",
        "              **kwargs) -> float:\n",
        "        batch_size: int = kwargs.get('batch_size', 50)\n",
        "        discount_factor: float = kwargs.get('discount_factor', 0.9999)\n",
        "        learning_rate: float = kwargs.get('learning_rate', 0.005)\n",
        "        eps_start: float = kwargs.get('eps_start', 0.9)\n",
        "        eps_end: float = kwargs.get('eps_end', 0.05)\n",
        "        eps_decay_steps: int = kwargs.get('eps_decay_steps', 200)\n",
        "        update_target_every: int = kwargs.get('update_target_every', 100)\n",
        "        memory_capacity: int = kwargs.get('memory_capacity', 500)\n",
        "        render_interval: int = kwargs.get('render_interval', 50)  # in steps, None for episode end renderers only\n",
        "        \n",
        "        \n",
        "        \n",
        "        if n_steps and not n_episodes:\n",
        "            n_episodes = np.iinfo(np.int32).max\n",
        "\n",
        "        print('====      AGENT ID: {}      ===='.format(self.id))\n",
        "\n",
        "\n",
        "        \n",
        "        memory = ReplayMemory(memory_capacity, transition_type=DQNTransition)\n",
        "        episode = 0\n",
        "        total_steps_done = 0\n",
        "        total_reward = 0\n",
        "        stop_training = False\n",
        "        while episode < n_episodes and not stop_training:\n",
        "            state = self.env.reset()\n",
        "            done = False\n",
        "            steps_done = 0\n",
        "\n",
        "            while not done:\n",
        "                threshold = eps_end + (eps_start - eps_end) * np.exp(-total_steps_done / eps_decay_steps)#diminishing the threshold\n",
        "                action = self.get_action(state, threshold=threshold) #select an action via exploration or exploitation.\n",
        "                next_state, reward, done, _ = self.env.step(action) #observe reward and next state.\n",
        "                memory.push(state, action, reward, next_state, done) #store the state,action, reward and next state in the memory.\n",
        "                state = next_state #update of state\n",
        "                total_reward += reward #update of total reward\n",
        "                steps_done += 1 #update of state.\n",
        "                total_steps_done +=1\n",
        "                if len(memory) < batch_size:\n",
        "                    continue\n",
        "                self.apply_gradient_descent(memory, batch_size, learning_rate, discount_factor)#The gradient descent function which minimizes the loss\n",
        "                if n_steps and steps_done >= n_steps:\n",
        "                    done = True\n",
        "\n",
        "                if render_interval is not None and steps_done % render_interval == 0:\n",
        "                    c = actionscheme.get_orders(action=action,portfolio=portfoliox)\n",
        "                    try:\n",
        "                      print(\"episode: {},action: {},time: {},quantity: {},price: {},score: {}\".format(episode,c[0].side,timeofaction(c[0].price),c[0].quantity,c[0].price,env.step(action)[3]))\n",
        "                    except IndexError:\n",
        "                      print(\"no action has been done in this timestep\")\n",
        "                    finally:\n",
        "                       print(\"episode: {},score: {}\".format(episode,env.step(action)[3]))\n",
        "\n",
        "                if steps_done % update_target_every == 0:\n",
        "                    self.target_network = tf.keras.models.clone_model(self.policy_network)\n",
        "                    self.target_network.trainable = False\n",
        "\n",
        "            is_checkpoint = save_every and episode % save_every == 0\n",
        "\n",
        "            if save_path and (is_checkpoint or episode == n_episodes - 1):\n",
        "                self.save(save_path, episode=episode)\n",
        "\n",
        "            if not render_interval or steps_done < n_steps:\n",
        "                  c = actionscheme.get_orders(action=action,portfolio=portfoliox)\n",
        "                  try:\n",
        "                    print(\"episode: {},action: {},time: {},quantity: {},price: {},score: {}\".format(episode,c[0].side,timeofaction(c[0].price),c[0].quantity,c[0].price,env.step(action)[3]))\n",
        "                  except IndexError:\n",
        "                    print(\"no action has been done in this timestep\") \n",
        "                  finally:\n",
        "                    print(\"episode: {},score: {}\".format(episode,env.step(action)[3]))\n",
        "            self.env.save()\n",
        "            episode += 1\n",
        "        mean_reward = total_reward / steps_done\n",
        "        return mean_reward \n",
        "       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqENz3A0ytEo"
      },
      "source": [
        "This is the agent training function just plug in the number of timesteps and the number of episodes you want the agent to train for and watch it trade everytime it did an action it will be printed out, if you see that an action has resulted in profit and would like to know the time and price of the currency at the time of the action just type in code timeofaction(price) to see how the agent trades, if however you felt confident in its ability to trade just type in code  restore(filepath of model) and it will restore the saved model.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_Boi4wuJ86Y"
      },
      "source": [
        "from tensortrade.agents import DQNAgent\n",
        "\n",
        "agent= DQNAgents(env)\n",
        "agent.policy_network.summary()\n",
        "#agent.train(n_steps=2000, n_episodes=20, save_path=\"agents/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mEnNfVKaRrv"
      },
      "source": [
        "Example of the time of an action, you observe the action after its done if a profit has been made after the action you can type time of action(price) to know the time in which that action has been made and therefore extract knowledge of how the agent has made profit.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_95B9l2aMp6",
        "outputId": "021d3f19-1447-429e-cb39-159036ed99ef"
      },
      "source": [
        "timeofaction(6295.10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2018-06-14 00:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND0psiy22stC"
      },
      "source": [
        "#save(\"\\agents\")\n",
        "#restore(\"\\agents\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}